{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53443557",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3078a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SparkML: Transformers and Estimators\n",
    "\n",
    "![SparkML](sparkml.webp)\n",
    "\n",
    "Rodrigo Agundez - 06 April 2023 - [SparkML Documentation](https://spark.apache.org/docs/latest/ml-pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a81423c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Main concepts\n",
    "\n",
    "- ## Transformer\n",
    "    A Transformer is an algorithm which can transform one DataFrame into another DataFrame.\n",
    "- ## Estimator\n",
    "    An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer.\n",
    "- ## Pipeline\n",
    "    A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a08714",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimator fit -> Transformer\n",
    "\n",
    "![SparkML Fit](sparkml_pipeline_fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68216b16",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Above, the top row represents a Pipeline with three stages. The first two (Tokenizer and HashingTF) are Transformers (blue), and the third (LogisticRegression) is an Estimator (red). The bottom row represents data flowing through the pipeline, where cylinders indicate DataFrames. The Pipeline.fit() method is called on the original DataFrame, which has raw text documents and labels. The Tokenizer.transform() method splits the raw text documents into words, adding a new column with words to the DataFrame. The HashingTF.transform() method converts the words column into feature vectors, adding a new column with those vectors to the DataFrame. Now, since LogisticRegression is an Estimator, the Pipeline first calls LogisticRegression.fit() to produce a LogisticRegressionModel. If the Pipeline had more Estimators, it would call the LogisticRegressionModel’s transform() method on the DataFrame before passing the DataFrame to the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bbabce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer transform\n",
    "\n",
    "![SparkML Transform](sparkml_pipeline_transform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c02bed",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the figure above, the PipelineModel has the same number of stages as the original Pipeline, but all Estimators in the original Pipeline have become Transformers. When the PipelineModel’s transform() method is called on a test dataset, the data are passed through the fitted pipeline in order. Each stage’s transform() method updates the dataset and passes it to the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd4e082",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Classifier of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae79935a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b284929",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build Pipeline\n",
    "\n",
    "![SparkML Pipeline Estimator](sparkml_pipeline_estimator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f99d5889",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_3847b0efdcec"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb911ad5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fit the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3206454d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](sparkml_pipeline_estimator_fit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ca121ad",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_c9e6dcc16c74"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_transformer = pipeline.fit(training)\n",
    "doc_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6598c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transform a Dataframe\n",
    "\n",
    "![](sparkml_pipeline_transformer_transform.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d18353",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|            text|label|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+----------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  0| a b c d e spark|  1.0|[a, b, c, d, e, s...|(262144,[74920,89...|[-5.9388192690225...|[0.00262821349694...|       1.0|\n",
      "|  1|             b d|  0.0|              [b, d]|(262144,[89530,14...|[5.62050636899133...|[0.99639027118011...|       0.0|\n",
      "|  2|     spark f g h|  1.0|    [spark, f, g, h]|(262144,[36803,17...|[-6.1134100250082...|[0.00220810505702...|       1.0|\n",
      "|  3|hadoop mapreduce|  0.0| [hadoop, mapreduce]|(262144,[132966,1...|[6.66214714866364...|[0.99872323370637...|       0.0|\n",
      "+---+----------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_transformer.transform(training).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf1395",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Save pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a7fd743",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doc_transformer.write().overwrite().save(\"doc_transformer.pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42262716",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transform another Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e2a3dac",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|              text|\n",
      "+---+------------------+\n",
      "|  4|       spark i j k|\n",
      "|  5|             l m n|\n",
      "|  6|spark hadoop spark|\n",
      "|  7|     apache hadoop|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "test = spark.createDataFrame([(4, \"spark i j k\"), (5, \"l m n\"), (6, \"spark hadoop spark\"), (7, \"apache hadoop\")], [\"id\", \"text\"])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2be42d7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|              text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|       spark i j k|    [spark, i, j, k]|(262144,[19036,68...|[0.52882855227968...|[0.62920984896684...|       0.0|\n",
      "|  5|             l m n|           [l, m, n]|(262144,[1303,526...|[4.16914139534005...|[0.98477000676230...|       0.0|\n",
      "|  6|spark hadoop spark|[spark, hadoop, s...|(262144,[173558,1...|[-1.8649814141188...|[0.13412348342566...|       1.0|\n",
      "|  7|     apache hadoop|    [apache, hadoop]|(262144,[68303,19...|[5.41564427200184...|[0.99557321143985...|       0.0|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_transformer = PipelineModel.load('doc_transformer.pipeline')\n",
    "doc_transformer.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1b129",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Custom Transformer and Estimator\n",
    "\n",
    "Is not easy.\n",
    "\n",
    "- [StackOverflow: Create a custom Transformer in PySpark ML](https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml)\n",
    "- [StackOverflow: Serialize a custom transformer using python to be used within a Pyspark ML pipeline](https://stackoverflow.com/questions/41399399/serialize-a-custom-transformer-using-python-to-be-used-within-a-pyspark-ml-pipel/44377489#44377489)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f727fdf0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# [Custom SparkML](04-custom_sparkml.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
