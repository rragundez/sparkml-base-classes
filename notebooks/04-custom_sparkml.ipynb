{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85bced49",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/agundrod/spark-3.3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/agundrod/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/agundrod/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d54e18bf-8233-41f2-a245-7a8de68cae60;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 150ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d54e18bf-8233-41f2-a245-7a8de68cae60\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 11:19:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/06 11:19:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|price|\n",
      "+---+-----+\n",
      "|  4|13.87|\n",
      "|  5| 23.5|\n",
      "|  6| 45.1|\n",
      "|  7|30.78|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.ml import Estimator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data = spark.createDataFrame([(4, 13.87), (5, 23.5), (6, 45.10), (7, 30.78)], [\"id\", \"price\"])\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9438b81a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Custom SparkML\n",
    "\n",
    "![SparkML](sparkml.webp)\n",
    "\n",
    "Rodrigo Agundez - 06 April 2023- [Tutorial in Medium](https://medium.com/@rragundez/easily-build-custom-sparkml-transformers-and-estimators-16ba70414abe)\n",
    "\n",
    "- [StackOverflow: Create a custom Transformer in PySpark ML](https://stackoverflow.com/questions/32331848/create-a-custom-transformer-in-pyspark-ml)\n",
    "- [StackOverflow: Serialize a custom transformer using python to be used within a Pyspark ML pipeline](https://stackoverflow.com/questions/41399399/serialize-a-custom-transformer-using-python-to-be-used-within-a-pyspark-ml-pipel/44377489#44377489)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25f64c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer: Divide column by a number\n",
    "\n",
    "The hard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad579e35",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DivideColumnTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    value = Param(Params._dummy(), \"value\", \"value\", typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, value=None):\n",
    "        super().__init__()\n",
    "        self.value = Param(self, \"value\", \"\")\n",
    "        self._setDefault(value=1)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    " \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, value=None):\n",
    "      \n",
    "\n",
    "    def setValue(self, value):\n",
    "        return self._set(value=float(value))\n",
    "\n",
    "    def getValue(self):\n",
    "        return self.getOrDefault(self.value)\n",
    "\n",
    "    def setInputCol(self, value):\n",
    "        return self._set(inputCol=value)\n",
    "\n",
    "    def setOutputCol(self, value):\n",
    "        return self._set(outputCol=value)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.col(self.getInputCol()) / self.getValue())\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2349eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's transform some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aee021d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|price|\n",
      "+---+-----+\n",
      "|  4|13.87|\n",
      "|  5| 23.5|\n",
      "|  6| 45.1|\n",
      "|  7|30.78|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06210621",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|price|\n",
      "+---+-----+\n",
      "|  4|6.935|\n",
      "|  5|11.75|\n",
      "|  6|22.55|\n",
      "|  7|15.39|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_transformer = DivideColumnTransformer(inputCol=\"price\", outputCol=\"price\", value=2)\n",
    "col_transformer.transform(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe05ca6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimator: Normalize by the mean\n",
    "\n",
    "The hard way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d715f5a6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class MeanNormalizerEstimator(Estimator, HasInputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None):\n",
    "        super().__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setInputCol(self, value):\n",
    "        return self._set(inputCol=value)\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        mean, = dataset.agg(F.mean(self.getInputCol())).first()\n",
    "        return DivideColumnTransformer(inputCol=self.getInputCol(), outputCol=self.getInputCol(), value=mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5833b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b30959",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "column_transformer = MeanNormalizerEstimator(inputCol=\"price\")\n",
    "pipeline = Pipeline(stages=[column_transformer])\n",
    "fitted_pipeline = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efac1a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5ee81a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 28.3125\n",
      "+---+------------------+\n",
      "| id|             price|\n",
      "+---+------------------+\n",
      "|  4|0.4898896247240618|\n",
      "|  5|0.8300220750551877|\n",
      "|  6|1.5929359823399558|\n",
      "|  7|1.0871523178807947|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean:\", fitted_pipeline.stages[0].getValue())\n",
    "fitted_pipeline.transform(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5adf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Save the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb11bf78",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fitted_pipeline.write().overwrite().save(\"fitted_pipeline.pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f98dc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load the fitted pipeline and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0467373",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|price|\n",
      "+---+-----+\n",
      "|  4|34.87|\n",
      "|  5| 33.5|\n",
      "|  6| 15.1|\n",
      "|  7|20.78|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "test = spark.createDataFrame([(4, 34.87), (5, 33.5), (6, 15.10), (7, 20.78)], [\"id\", \"price\"])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc33664",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable  \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "data = spark.createDataFrame([(4, 13.87), (5, 23.5), (6, 45.10), (7, 30.78)], [\"id\", \"price\"])\n",
    "\n",
    "class DivideColumnTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    value = Param(Params._dummy(), \"value\", \"value\", typeConverter=TypeConverters.toFloat)\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, value=None):\n",
    "        super().__init__()\n",
    "        self.value = Param(self, \"value\", \"\")\n",
    "        self._setDefault(value=1)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, value=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def setValue(self, value):\n",
    "        return self._set(value=float(value))\n",
    "\n",
    "    def getValue(self):\n",
    "        return self.getOrDefault(self.value)\n",
    "\n",
    "    def setInputCol(self, value):\n",
    "        return self._set(inputCol=value)\n",
    "\n",
    "    def setOutputCol(self, value):\n",
    "        return self._set(outputCol=value)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        dataset = dataset.withColumn(self.getOutputCol(), F.col(self.getInputCol()) / self.getValue())\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d44936",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 28.3125\n",
      "+---+------------------+\n",
      "| id|             price|\n",
      "+---+------------------+\n",
      "|  4|1.2316114790286974|\n",
      "|  5|1.1832229580573952|\n",
      "|  6|0.5333333333333333|\n",
      "|  7|0.7339514348785873|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitted_pipeline = PipelineModel.load('fitted_pipeline.pipeline')\n",
    "print(\"Mean:\", fitted_pipeline.stages[0].getValue())\n",
    "fitted_pipeline.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9d1bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The easy way\n",
    "\n",
    "[SparkML Base Clases in Pypi](https://pypi.org/project/sparkml-base-classes/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6257eabc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: sparkml-base-classes 0.1.6\n",
      "Uninstalling sparkml-base-classes-0.1.6:\n",
      "  Successfully uninstalled sparkml-base-classes-0.1.6\n",
      "Using pip 22.0.4 from /Users/agundrod/.pyenv/versions/3.9.11/lib/python3.9/site-packages/pip (python 3.9)\n",
      "Collecting sparkml-base-classes\n",
      "  Downloading sparkml_base_classes-0.1.6-py3-none-any.whl (6.1 kB)\n",
      "Installing collected packages: sparkml-base-classes\n",
      "Successfully installed sparkml-base-classes-0.1.6\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/agundrod/.pyenv/versions/3.9.11/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall sparkml-base-classes -y\n",
    "!pip install sparkml-base-classes --no-cache-dir -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054d00c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformer: Divide column by a number (The easy way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d361038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|price|\n",
      "+---+-----+\n",
      "|  4|34.87|\n",
      "|  5| 33.5|\n",
      "|  6| 15.1|\n",
      "|  7|20.78|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fc80176",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.sql import functions as F\n",
    "from sparkml_base_classes import TransformerBaseClass\n",
    "\n",
    "class DivideColumnTransformer(TransformerBaseClass):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, column_name=None, value=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, ddf):\n",
    "        ddf = ddf.withColumn(self._column_name, F.col(self._column_name) / self._value)\n",
    "        return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0c3d188",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id| price|\n",
      "+---+------+\n",
      "|  4|17.435|\n",
      "|  5| 16.75|\n",
      "|  6|  7.55|\n",
      "|  7| 10.39|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_transformer = DivideColumnTransformer(column_name=\"price\", value=2)\n",
    "col_transformer.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99c562",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Estimator: Normalize by the mean\n",
    "\n",
    "The easy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa60e4a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sparkml_base_classes import EstimatorBaseClass\n",
    "\n",
    "class MeanNormalizerEstimator(EstimatorBaseClass):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, column_name=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def _fit(self, ddf):\n",
    "        mean, = ddf.agg(F.mean(self._column_name)).first()\n",
    "        return DivideColumnTransformer(\n",
    "            column_name=self._column_name,\n",
    "            value=mean\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "236e00b4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "column_transformer = MeanNormalizerEstimator(column_name=\"price\")\n",
    "pipeline = Pipeline(stages=[column_transformer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5266ff7f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3add6e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 28.3125\n",
      "+---+------------------+\n",
      "| id|             price|\n",
      "+---+------------------+\n",
      "|  4|0.4898896247240618|\n",
      "|  5|0.8300220750551877|\n",
      "|  6|1.5929359823399558|\n",
      "|  7|1.0871523178807947|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitted_pipeline = pipeline.fit(data)\n",
    "print(\"Mean:\", fitted_pipeline.stages[0]._value)\n",
    "fitted_pipeline.transform(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd253528",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Save the fitted pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "368c86db",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fitted_pipeline.write().overwrite().save(\"custom_fitted_pipeline.pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea77ff7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load the fitted pipeline and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d40f0a46",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|price|\n",
      "+---+-----+\n",
      "|  4|34.87|\n",
      "|  5| 33.5|\n",
      "|  6| 15.1|\n",
      "|  7|20.78|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "test = spark.createDataFrame([(4, 34.87), (5, 33.5), (6, 15.10), (7, 20.78)], [\"id\", \"price\"])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a99794e9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.sql import functions as F\n",
    "from sparkml_base_classes import TransformerBaseClass\n",
    "\n",
    "class DivideColumnTransformer(TransformerBaseClass):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, column_name=None, value=None):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform(self, ddf):\n",
    "        ddf = ddf.withColumn(self._column_name, F.col(self._column_name) / self._value)\n",
    "        return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5ea6c0f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 28.3125\n",
      "+---+------------------+\n",
      "| id|             price|\n",
      "+---+------------------+\n",
      "|  4|1.2316114790286974|\n",
      "|  5|1.1832229580573952|\n",
      "|  6|0.5333333333333333|\n",
      "|  7|0.7339514348785873|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fitted_pipeline = PipelineModel.load('custom_fitted_pipeline.pipeline')\n",
    "print(\"Mean:\", fitted_pipeline.stages[0]._value)\n",
    "fitted_pipeline.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092c3b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Example: OneHot Encoding in aditrade](http://aditrade.aws.3stripes.net/tmp_source/aditrade.etl.one_hot_estimator.html)\n",
    "\n",
    "# [Example: PCA in aditrade](http://aditrade.aws.3stripes.net/tmp_source/aditrade.etl.principal_component_analysis.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f6f41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Source Code of SparkML Base Classes](https://github.com/rragundez/sparkml-base-classes)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
